# deeplearning_notes

##[第一周] introduction to deep learning

### 1.1 欢迎来到深度学习工程师微专业
### 1.2 什么是神经网络？

### 1.3 用神经网络进行监督学习

* 三种监督学习
> standard NN、Convolutional NN、Recurrent NN

* structured data & unstructured data
> 有否清晰的定义（）。音频、图像、文字就是非结构化数据。

### 1.4 为什么深度学习会兴起？
> 数据规模影响神经网络的训练。训练集不大的时候，机器学习其它算法都差不多。训练集大的时候，NN有优势
![](images/屏幕快照 2017-10-24 下午1.02.00.png)

* data
* computation
* algorithms
![](images/屏幕快照 2017-10-25 上午12.28.15.png)
> 计算能力越快

### 1.5 关于这门课

## [第二周]神经网络基础

### 2.1 二分分类（binary classification）

![](images/屏幕快照 2017-10-25 上午12.39.18.png)

### 2.2 logistic回归

> 已知x。求y_hat=P(y=1|x)
> output--> y_hat=sigmoid(w_T*x+b)
>sigmoid(z) =1/(1+e^(-z))
>

### 2.3 logistic回归损失函数

### 2.4

### 2.3 logistic 回归损失函数

### 2.4 梯度下降法

### 2.5 导数

### 2.6 更多导数的例子

### 2.7 计算图

### 2.8 计算图的导数计算

### 2.9 logistic 回归中的梯度下降法

### 2.10 m 个样本的梯度下降

### 2.11 向量化

### 2.12 向量化的更多例子

### 2.13 向量化 logistic 回归

### 2.14 向量化 logistic 回归的梯度输出

### 2.15 Python 中的广播

### 2.16 关于 python / numpy 向量的说明

### 2.17 Jupyter / Ipython 笔记本的快速指南

### 2.18 （选修）logistic 损失函数的解释

## [第三周]浅层神经网络

### 3.1 神经网络概览

### 3.2 神经网络表示

### 3.3 计算神经网络的输出

### 3.4 多个例子中的向量化

### 3.5 向量化实现的解释

### 3.6 激活函数

### 3.7 为什么需要非线性激活函数？

### 3.8 激活函数的导数

### 3.9 神经网络的梯度下降法

### 3.10 （选修）直观理解反向传播

### 3.11 随机初始化

## [第四周]深层神经网络

### 4.1 深层神经网络

### 4.2 深层网络中的前向传播

### 4.3 核对矩阵的维数

### 4.4 为什么使用深层表示

### 4.5 搭建深层神经网络块

### 4.6 前向和反向传播

### 4.7 参数 VS 超参数

### 4.8 这和大脑有什么关系？

